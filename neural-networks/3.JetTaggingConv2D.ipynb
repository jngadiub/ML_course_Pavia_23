{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7_esAc6K1Z4"
      },
      "source": [
        "# Training a Jet Tagging with **Conv2D** \n",
        "\n",
        "---\n",
        "In this notebook, we perform a Jet identification task using a Conv2D multiclass classifier.\n",
        "The problem consists in identifying a given jet as a quark, a gluon, a W, a Z, or a top,\n",
        "based on a jet image, i.e., a 2D histogram of the transverse momentum ($p_T$) deposited in each of 100x100\n",
        "bins of a square window of the ($\\eta$, $\\phi$) plane, centered along the jet axis.\n",
        "\n",
        "For details on the physics problem, see https://arxiv.org/pdf/1804.06913.pdf \n",
        "\n",
        "For details on the dataset, see Notebook1\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pK5FCdMK1Z7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import h5py\n",
        "import glob, pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVO0EQALK1Z8"
      },
      "source": [
        "# Preparation of the training and validation samples\n",
        "\n",
        "---\n",
        "In order to import the dataset, we now\n",
        "- clone the dataset repository (to import the data in Colab)\n",
        "- load the h5 files in the data/ repository\n",
        "- extract the data we need: a target and jetImage \n",
        "\n",
        "To type shell commands, we start the command line with !\n",
        "\n",
        "**nb, if you are running locally and you have already downloaded the datasets you can skip the cell below and, if needed, change the paths later to point to the folder with your previous download of the datasets.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlxKaXA8K1Z-"
      },
      "outputs": [],
      "source": [
        "! curl https://cernbox.cern.ch/s/6Ec5pGFEpFWeH6S/download -o Data-MLtutorial.tar.gz\n",
        "! tar -xvzf Data-MLtutorial.tar.gz \n",
        "! ls Data-MLtutorial/JetDataset/\n",
        "! rm Data-MLtutorial.tar.gz "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Bfy4kz2K1Z_"
      },
      "outputs": [],
      "source": [
        "target = np.array([])\n",
        "jetImage = np.array([])\n",
        "# we cannot load all data on Colab. So we just take a few files\n",
        "datafiles = ['Data-MLtutorial/JetDataset/jetImage_7_100p_30000_40000.h5',\n",
        "             'Data-MLtutorial/JetDataset/jetImage_7_100p_60000_70000.h5',\n",
        "             'Data-MLtutorial/JetDataset/jetImage_7_100p_50000_60000.h5',\n",
        "             'Data-MLtutorial/JetDataset/jetImage_7_100p_10000_20000.h5',\n",
        "             'Data-MLtutorial/JetDataset/jetImage_7_100p_0_10000.h5']\n",
        "# if you are running locally, you can use the full dataset doing\n",
        "# for fileIN in glob.glob(\"tutorials/HiggsSchool/data/*h5\"):\n",
        "for fileIN in datafiles:\n",
        "    print(\"Appending %s\" %fileIN)\n",
        "    f = h5py.File(fileIN)\n",
        "    myjetImage = np.array(f.get(\"jetImage\"))\n",
        "    mytarget = np.array(f.get('jets')[0:,-6:-1])\n",
        "    jetImage = np.concatenate([jetImage, myjetImage], axis=0) if jetImage.size else myjetImage\n",
        "    target = np.concatenate([target, mytarget], axis=0) if target.size else mytarget\n",
        "    f.close()\n",
        "print(target.shape, jetImage.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTuZKlYlK1aA"
      },
      "source": [
        "The dataset consists of 50000 with up to 100 particles in each jet. These 100 particles have been used to fill the 100x100 jet images.\n",
        "\n",
        "---\n",
        "\n",
        "We now shuffle the data, splitting them into a training and a validation dataset with 2:1 ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZenCl_lK1aC"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(jetImage, target, test_size=0.33)\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
        "del jetImage, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRONFD2bK1aD"
      },
      "source": [
        "In keras, images are representable as $n \\times m \\times k$ tensors, where $n \\times m$ are the pixel dimenions and $k$ is the number of channels (e.g., 1 in a black\\&while image, 3 for an RGB image). In our case, k=1. To comply to this, we add the channel index by reshaping the image dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwzpamYuK1aE"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
        "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], X_val.shape[2], 1))\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cj7HnaAK1aF"
      },
      "source": [
        "# Conv 2D model building\n",
        "\n",
        "The main ingredients of a Conv2D layer are:\n",
        "\n",
        "- **filter**: a *k x k’* matrix of weights (orange matrix in the picture below) that scans the image and performs a scalar product of each image block (this is also called *kernel*)\n",
        "- **stride**: number of pixels the filter is shifted by (=1 in the image below)\n",
        "- **padding**: the amount of pixels added to an image when it is being processed by the filter of a CNN (helps keeping information on the boundaries of the original image by allowing border pixels to be at the center of the filter)\n",
        "    - *valid* means no padding (default setting)\n",
        "    - *same* results in padding with zeros evenly to the left/right or up/down of the input image as needed to ensure that the output has the same shape as the input\n",
        "\n",
        "<div>\n",
        "<img src=\"https://raw.githubusercontent.com/jngadiub/ML_course_Pavia_23/main/neural-networks/conv2d.gif\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "It is common practice to insert **pooling** layers in between Conv2D layers to progressively reduce the size of the representation and thus reduce the amount of parameters and computation in the network. Pooling also makes processing more robust to changes in the position of a feature in the image. Common types of pooling operations are:\n",
        "\n",
        "- **MaxPooling**: given an image and a pool of size *k x k’*, scans the image and replaces each *k x k’* patch with its *maximum* -- helps to extract the sharpest features on the image when the sharpest features are a best lower-level representation of the image\n",
        "- **AveragePooling**: given an image and a pool of size *k x k’*, scans the image and replaces each *k x k’* patch with its *average* -- helps to extract the smooth features when \"colours\" transition is smooth\n",
        "\n",
        "<img src=\"https://github.com/jngadiub/ML_course_Pavia_23/blob/main/neural-networks/figures/pooling.png?raw=1\" alt=\"Drawing\" style=\"width: 800px;\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaUzTTuYK1aG"
      },
      "outputs": [],
      "source": [
        "# keras imports\n",
        "from tensorflow.keras.models import Model, model_from_json\n",
        "from tensorflow.keras.layers import Dense, Input, Conv2D, Dropout, Flatten\n",
        "from tensorflow.keras.layers import MaxPooling2D, BatchNormalization, Activation\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evIemCVrK1aH"
      },
      "outputs": [],
      "source": [
        "img_rows = X_train.shape[1]\n",
        "img_cols = X_train.shape[2]\n",
        "dropoutRate = 0.25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjQH0sMuK1aI"
      },
      "outputs": [],
      "source": [
        "image_shape = (img_rows, img_cols, 1)\n",
        "####\n",
        "inputImage = Input(shape=(image_shape))\n",
        "x = Conv2D(5, kernel_size=(5,5), data_format=\"channels_last\", strides=(1, 1), padding=\"same\")(inputImage)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = MaxPooling2D( pool_size = (5,5))(x)\n",
        "x = Dropout(dropoutRate)(x)\n",
        "#\n",
        "x = Conv2D(3, kernel_size=(3,3), data_format=\"channels_last\", strides=(1, 1), padding=\"same\")(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = MaxPooling2D( pool_size = (3,3))(x)\n",
        "x = Dropout(dropoutRate)(x)\n",
        "#\n",
        "x = Flatten()(x)\n",
        "#\n",
        "x = Dense(5, activation='relu')(x)\n",
        "#\n",
        "output = Dense(5, activation='softmax')(x)\n",
        "####\n",
        "model = Model(inputs=inputImage, outputs=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXVPnzcTK1aI"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5leMyRciK1aJ"
      },
      "source": [
        "We now train the model. This takes really long time and processing power on common CPUs. **If you are running locally set TRAIN=False** such that a pre-trained model is loaded for the next evaluation steps. We live as homework to reproduce the results (suggest to use Colab with GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj_vANK5K1aK"
      },
      "outputs": [],
      "source": [
        "TRAIN = False\n",
        "batch_size = 128\n",
        "n_epochs = 10\n",
        "\n",
        "if TRAIN: #train and save the model\n",
        "    \n",
        "    history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size, verbose = 2,\n",
        "                        validation_data=(X_val, y_val),\n",
        "                        callbacks = [\n",
        "                        EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
        "                        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1),\n",
        "                        TerminateOnNaN()])\n",
        "\n",
        "    model_json = model.to_json()\n",
        "    outputdir = './jetTagger_CNN/'\n",
        "\n",
        "    with open(\"{OUTPUTDIR}/jetTagger_CNN.json\".format(OUTPUTDIR=outputdir), \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "    model.save_weights(\"{OUTPUTDIR}/jetTagger_CNN.h5\".format(OUTPUTDIR=outputdir))\n",
        "    \n",
        "    with open('{OUTPUTDIR}/history.h5'.format(OUTPUTDIR=outputdir), 'wb') as f:\n",
        "      pickle.dump(history.history, f, protocol=pickle.HIGHEST_PROTOCOL)    \n",
        "    \n",
        "else: #load pretrained model\n",
        "    \n",
        "    ! curl https://cernbox.cern.ch/index.php/s/yYUgxxSnYN42qay/download -o jetTagger_CNN.tar.gz\n",
        "    ! tar -xvzf jetTagger_CNN.tar.gz \n",
        "    ! ls jetTagger_CNN/\n",
        "    ! rm jetTagger_CNN.tar.gz\n",
        "    \n",
        "    with open('jetTagger_CNN/jetTagger_CNN.json', 'r') as json_file:\n",
        "        model_json = json_file.read()\n",
        "    model = model_from_json(model_json)\n",
        "    model.load_weights(\"jetTagger_CNN/jetTagger_CNN.h5\")\n",
        "    \n",
        "    with open('jetTagger_CNN/history.h5', 'rb') as f: history = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7E3JsM_K1aK"
      },
      "outputs": [],
      "source": [
        "# plot training history\n",
        "if TRAIN: history = pd.DataFrame(history.history)\n",
        "plt.plot(history['loss'])\n",
        "plt.plot(history['val_loss'])\n",
        "plt.yscale('log')\n",
        "plt.title('Training History')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training', 'validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVNnL-1QK1aM"
      },
      "source": [
        "# Building the ROC Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWbtbTrPK1aM"
      },
      "outputs": [],
      "source": [
        "labels = ['gluon', 'quark', 'W', 'Z', 'top']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N33vdzeQK1aM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "predict_val = model.predict(X_val)\n",
        "df = pd.DataFrame()\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "auc1 = {}\n",
        "\n",
        "plt.figure()\n",
        "for i, label in enumerate(labels):\n",
        "        df[label] = y_val[:,i]\n",
        "        df[label + '_pred'] = predict_val[:,i]\n",
        "\n",
        "        fpr[label], tpr[label], threshold = roc_curve(df[label],df[label+'_pred'])\n",
        "\n",
        "        auc1[label] = auc(fpr[label], tpr[label])\n",
        "\n",
        "        plt.plot(tpr[label],fpr[label],label='%s tagger, auc = %.1f%%'%(label,auc1[label]*100.))\n",
        "plt.semilogy()\n",
        "plt.xlabel(\"sig. efficiency\")\n",
        "plt.ylabel(\"bkg. mistag rate\")\n",
        "plt.ylim(0.000001,1)\n",
        "plt.grid(True)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4zyw_ExK1aN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}